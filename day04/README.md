# ğŸ•¸ Day 4 â€“ Web Spidering with wget

Today weâ€™re using `wget` to recursively crawl and mirror websites â€” perfect for locating hidden paths, admin panels, and backup files.

---

## ğŸš€ What Youâ€™ll Learn

- How to use `wget` for recursive site crawling
- How to mirror an entire website structure
- Where to look for interesting content in downloaded data

---

## âš™ï¸ Instructions

1. Open your terminal in Kali.
2. Navigate to the Day 04 script directory:
   ```bash
   cd day04/scripts
   ./webspider.sh

3. When prompted, enter the target domain (e.g. example.com).

ğŸ“‚ Output will be saved in:
day04/web-mirror/

ğŸ§° Tools Used
wget â€“ recursive web spidering

bash â€“ CLI script automation

ğŸ¯ Pro Tips
Pair this with grep to search downloaded files for juicy keywords (admin, login, backup).

Try combining this with dirsearch or ffuf in later challenges for full enumeration.

ğŸ¥ Built for TikTok + Terminal Capture
Perfect with OBS + svg-term-cli + high-contrast themes

ğŸ›¡ï¸ Ethical Use Only â€“ Built for education and awareness.

## ğŸ•¸ï¸ wget Spidering Demo

Live terminal screenshots of `webspider.sh`:

![Scan Start](webspider_scan.png)
![Terminal View](webspider_terminal_view.png)
![Scan Complete](webspider_scan_complete.png)
![Final Files](webspider_finished.png)
